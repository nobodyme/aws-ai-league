"""Evaluate model responses produced by 5-inference.py.

This script expects the CSV generated by 5-inference.py containing the columns
``instruction``, ``context``, ``expected_answer`` and one column per model
label. It computes BLEU-2, BLEU-4, and normalized Levenshtein similarity scores
for every model on each example, writes comparison tables to disk, and draws
boxplots similar to the legacy notebook.
"""

from __future__ import annotations

import re
from concurrent.futures import ProcessPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
from bert_score import score as _bert_score
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from multiprocessing import cpu_count


_SMOOTH_FN = SmoothingFunction().method3
_BERT_SCORE_DEFAULT_KWARGS = {
    "lang": "en",
    "rescale_with_baseline": True,
}
_BERT_SCORE_MODEL_TYPE = "microsoft/deberta-base-mnli"


def tokenize(text: str) -> List[str]:
    return re.findall(r"[A-Za-z0-9']+", (text or "").lower())


def _sanitize_text(value: object) -> str:
    if isinstance(value, str):
        return value
    if value is None:
        return ""
    if pd.isna(value):
        return ""
    return str(value)


def _row_metrics_worker(args: Tuple[List[str], Dict[str, object]]) -> Dict[str, float]:
    model_cols, record = args
    expected = _sanitize_text(record.get("expected_answer"))

    entry: Dict[str, float] = {}
    for label in model_cols:
        prediction = _sanitize_text(record.get(label))
        scores = compute_metrics(expected, prediction)
        entry[f"{label}_bleu2"] = scores["bleu2"]
        entry[f"{label}_bleu4"] = scores["bleu4"]
        entry[f"{label}_levenshtein"] = scores["levenshtein"]

    return entry


def compute_bleu(reference: str, prediction: str) -> Dict[str, float]:
    ref_tokens = tokenize(reference)
    hyp_tokens = tokenize(prediction)
    bleu2 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5), smoothing_function=_SMOOTH_FN)
    bleu4 = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=_SMOOTH_FN)
    return {"bleu2": float(bleu2), "bleu4": float(bleu4)}


def get_model_labels(df: pd.DataFrame) -> List[str]:
    return [col for col in df.columns if col not in {"instruction", "context", "expected_answer"}]


def normalized_levenshtein(reference: str, prediction: str) -> float:
    ref = reference or ""
    hyp = prediction or ""
    if ref == hyp:
        return 1.0
    if not ref and not hyp:
        return 1.0
    if not ref or not hyp:
        return 0.0

    prev_row = list(range(len(hyp) + 1))
    for i, ref_char in enumerate(ref, start=1):
        current_row = [i]
        for j, hyp_char in enumerate(hyp, start=1):
            insertions = prev_row[j] + 1
            deletions = current_row[j - 1] + 1
            substitutions = prev_row[j - 1] + (ref_char != hyp_char)
            current_row.append(min(insertions, deletions, substitutions))
        prev_row = current_row

    distance = prev_row[-1]
    max_len = max(len(ref), len(hyp))
    if max_len == 0:
        return 1.0
    return 1.0 - (distance / max_len)


def compute_metrics(reference: str, prediction: str) -> Dict[str, float]:
    scores = compute_bleu(reference, prediction)
    scores["levenshtein"] = normalized_levenshtein(reference, prediction)
    return scores


def build_per_example_metrics(
    df: pd.DataFrame,
    model_cols: List[str] | None = None,
    *,
    include_bert_score: bool = True,
) -> pd.DataFrame:
    if model_cols is None:
        model_cols = get_model_labels(df)

    if df.empty:
        return pd.DataFrame()

    subset_cols = ["expected_answer", *model_cols]
    records = df[subset_cols].to_dict("records")
    if not records:
        return pd.DataFrame()

    worker_inputs: List[Tuple[List[str], Dict[str, object]]] = [(model_cols, record) for record in records]
    max_workers = min(len(worker_inputs), cpu_count() or 1)

    # Parallelize per-row metric computation to improve throughput on large evaluations.
    if max_workers > 1:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            metrics = list(executor.map(_row_metrics_worker, worker_inputs))
    else:
        metrics = [_row_metrics_worker(args) for args in worker_inputs]

    per_example_df = pd.DataFrame(metrics)

    label_order = [
        col
        for col in model_cols
        if any(
            metric in per_example_df.columns
            for metric in (
                f"{col}_bleu2",
                f"{col}_bleu4",
                f"{col}_levenshtein",
                f"{col}_bertscore_precision",
                f"{col}_bertscore_recall",
                f"{col}_bertscore_f1",
            )
        )
    ]
    bleu2_order = [f"{label}_bleu2" for label in label_order if f"{label}_bleu2" in per_example_df.columns]
    bleu4_order = [f"{label}_bleu4" for label in label_order if f"{label}_bleu4" in per_example_df.columns]
    levenshtein_order = [f"{label}_levenshtein" for label in label_order if f"{label}_levenshtein" in per_example_df.columns]

    bert_f1_order: List[str] = []
    if include_bert_score:
        references = [_sanitize_text(value) for value in df["expected_answer"].tolist()]
        bert_kwargs = dict(_BERT_SCORE_DEFAULT_KWARGS)
        if _BERT_SCORE_MODEL_TYPE:
            bert_kwargs["model_type"] = _BERT_SCORE_MODEL_TYPE
        for label in model_cols:
            if label not in df.columns:
                continue
            predictions = [_sanitize_text(value) for value in df[label].tolist()]
            if not predictions:
                continue
            _, _, f1 = _bert_score(predictions, references, **bert_kwargs)
            per_example_df[f"{label}_bertscore_f1"] = [float(x) for x in f1.tolist()]

        bert_f1_order = [
            f"{label}_bertscore_f1" for label in model_cols if f"{label}_bertscore_f1" in per_example_df.columns
        ]

    ordered_cols = bleu2_order + bleu4_order + levenshtein_order + bert_f1_order

    remaining_cols = [col for col in per_example_df.columns if col not in ordered_cols]
    if ordered_cols:
        per_example_df = per_example_df[ordered_cols + remaining_cols]

    return per_example_df


def plot_metric_boxplots(per_example_df: pd.DataFrame, output_path: Path) -> None:
    import matplotlib.pyplot as plt
    import seaborn as sns

    bleu4_cols = [col for col in per_example_df.columns if col.endswith("_bleu4")]
    lev_cols = [col for col in per_example_df.columns if col.endswith("_levenshtein")]
    if not bleu4_cols and not lev_cols:
        return

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if bleu4_cols:
        bleu_df = per_example_df.melt(
            value_vars=bleu4_cols,
            var_name="model",
            value_name="score",
        )
        bleu_df["model"] = bleu_df["model"].str.replace("_bleu4", "", regex=False)
        sns.boxplot(ax=axes[0], data=bleu_df, x="model", y="score")
        axes[0].set_xlabel("Model")
        axes[0].set_ylabel("BLEU-4")
        axes[0].set_title("BLEU-4 Score Distribution")
        axes[0].tick_params(axis="x", rotation=45)
    else:
        axes[0].axis("off")
        axes[0].set_title("No BLEU-4 data available")

    if lev_cols:
        lev_df = per_example_df.melt(
            value_vars=lev_cols,
            var_name="model",
            value_name="score",
        )
        lev_df["model"] = lev_df["model"].str.replace("_levenshtein", "", regex=False)
        sns.boxplot(ax=axes[1], data=lev_df, x="model", y="score")
        axes[1].set_xlabel("Model")
        axes[1].set_ylabel("Normalized Levenshtein Similarity")
        axes[1].set_ylim(0, 1)
        axes[1].set_title("Levenshtein Similarity Distribution")
        axes[1].tick_params(axis="x", rotation=45)
    else:
        axes[1].axis("off")
        axes[1].set_title("No Levenshtein data available")

    plt.tight_layout()
    fig.savefig(output_path)
    plt.close(fig)


if __name__ == "__main__":
    INCLUDE_BERT_SCORE = False

    RESPONSES_PATH = Path("logs/inference_runs/latest/model_responses.csv")
    OUTPUT_DIR = Path("logs") / "evaluations"

    if not RESPONSES_PATH.exists():
        raise SystemExit(f"Responses CSV not found: {RESPONSES_PATH}")

    responses_df = pd.read_csv(RESPONSES_PATH)
    model_labels = get_model_labels(responses_df)
    per_example_df = build_per_example_metrics(
        responses_df,
        model_labels,
        include_bert_score=INCLUDE_BERT_SCORE,
    )
    from zoneinfo import ZoneInfo

    timestamp = datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y%m%d-%H%M%S")
    run_dir = OUTPUT_DIR / timestamp
    run_dir.mkdir(parents=True, exist_ok=True)

    per_example_path = run_dir / "per_example_metrics.csv"
    per_example_df.to_csv(per_example_path, index=False)

    plot_metric_boxplots(per_example_df, run_dir / "metric_boxplots.png")

    print(f"Evaluation artifacts written to: {run_dir}")
