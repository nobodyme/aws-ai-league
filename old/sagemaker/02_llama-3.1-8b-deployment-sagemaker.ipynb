{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment of Fine-Tuned Llama 3.1 8B in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to deploy the fine-tuned model from the notebook [01_llama-3.1-8b-qlora-sft.ipynb](./01_llama-3.1-8b-qlora-sft.ipynb) in an Amazon SageMaker real-time endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "JupyterLab Instance Type: ml.t3.medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libriaries, including the Hugging Face libraries, and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q -U boto3\n",
    "%pip install -q -U botocore\n",
    "%pip install -q -U Levenshtein\n",
    "%pip install -q -U scikit-learn==1.5.1\n",
    "%pip install -q -U seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup Configuration file path\n",
    "\n",
    "We are setting the directory in which the config.yaml file resides so that remote decorator can make use of the settings through [SageMaker Defaults](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk).\n",
    "\n",
    "This notebook is using the Hugging Face container for the `us-east-1` region. Make sure you are using the right image for your AWS region, otherwise edit [config.yaml](./config.yaml). Container Images are available [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_prefix = f\"train-{model_id.split('/')[-1].replace('.', '-')}-auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefix):\n",
    "    import boto3\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "    search_response = sagemaker_client.search(\n",
    "        Resource='TrainingJob',\n",
    "        SearchExpression={\n",
    "            'Filters': [\n",
    "                {\n",
    "                    'Name': 'TrainingJobName',\n",
    "                    'Operator': 'Contains',\n",
    "                    'Value': job_name_prefix\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'TrainingJobStatus',\n",
    "                    'Operator': 'Equals',\n",
    "                    'Value': \"Completed\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1)\n",
    "\n",
    "    return search_response['Results'][0]['TrainingJob']['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = get_last_job_name(job_prefix)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Inference configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.4.0-tgi2.3.1-gpu-py311-cu124-ubuntu22.04-v2.0', #image_uri with py311\n",
    "    model_data=f\"s3://{bucket_name}/{job_name}/{job_name}/output/model.tar.gz\",\n",
    "    role=get_execution_role(),\n",
    "    env={\n",
    "        'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "        'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "        'HF_MODEL_QUANTIZE': 'bitsandbytes',\n",
    "        'MAX_INPUT_LENGTH': '4096',\n",
    "        'MAX_TOTAL_TOKENS': '8192'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    model_data_download_timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Fine-tuned model vs. Base model\n",
    "\n",
    "We are going to evaluate the fine-tuned model and the base model on two metrics:\n",
    "* BLEU Score\n",
    "* Accuracy score with Levenshtein distance\n",
    "\n",
    "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\n",
    "\n",
    "\n",
    "Normalized Levenshtein distance is an algorithm for evaluating accuracy degree of how close the calculated or measured values are to the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<ENDPOINT_NAME>\" #Required if you want to create a predictor without running the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predictor' not in locals() and 'predictor' not in globals():\n",
    "    print(\"Create predictor\")\n",
    "    predictor = HuggingFacePredictor(\n",
    "        endpoint_name=endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Amazon Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_model_id = \"meta.llama3-8b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./sample_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, valid = train_test_split(train, test_size=10, random_state=42)\n",
    "\n",
    "print(\"Number of validation elements: \", len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us first do a small test...\n",
    "row = valid.iloc[0]\n",
    "row['DESC_DEFECT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_defect_request_body(row, top_p=0.9, temperature=0.2, max_new_tokens=512):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>These are the information related to the defect:\\nManufacturer: {row['MFGNAME']}\\nComponent: {row['COMPNAME']}\\nDescription of the defect: {row['DESC_DEFECT']}\\n\\n\\nWhat are the consequences of defect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "    return {\n",
    "        'inputs': prompt,\n",
    "        'parameters': {\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"return_full_text\": False,\n",
    "            \"stop\": [\n",
    "                '<|eot_id|>',\n",
    "                '<|end_of_text|>'\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(build_defect_request_body(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "evaluation_set = []\n",
    "\n",
    "for index, row in valid.iterrows():\n",
    "    print(\"Example \", index)\n",
    "\n",
    "    ## Generate response with the fine-tuned model\n",
    "    body = build_defect_request_body(row)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = predictor.predict(body)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Generated response with fine-tuned model: {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "    response_fine_tuned = response[0]['generated_text'].strip()\n",
    "\n",
    "    print(response_fine_tuned)\n",
    "\n",
    "    ## Generate response with the base model\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        These are the information related to the defect:\n",
    "        Manufacturer: {row['MFGNAME']}\n",
    "        Component: {row['COMPNAME']}\n",
    "        Description of a defect:\n",
    "        {row['DESC_DEFECT']}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt},\n",
    "                        {\"text\": \"What are the consequences?\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=bedrock_model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.2,\n",
    "            \"topP\": 0.9,\n",
    "            \"maxTokens\": 512\n",
    "        }\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Generated response with base model: {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "    response_base = response['output']['message'][\"content\"][0][\"text\"]\n",
    "    print(response_base)\n",
    "\n",
    "    evaluation_set.append({\n",
    "        \"index\": index,\n",
    "        \"target_answer\": row[\"CONEQUENCE_DEFECT\"],\n",
    "        \"fine_tuned_answer\": response_fine_tuned,\n",
    "        \"base_answer\": response_base\n",
    "    })\n",
    "\n",
    "    print(\"******************\")\n",
    "\n",
    "with open(\"llama_32_1b_evaluation_dataset.json\", \"w\") as f:\n",
    "    json.dump(evaluation_set, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def clean_array(string):\n",
    "    filtered_words = []\n",
    "    \n",
    "    for element in string:\n",
    "        cleaned_word = re.sub(r'[^a-zA-Z]', '', element)\n",
    "        if cleaned_word:\n",
    "            filtered_words.append(cleaned_word)\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "def calculate_score(index, reference, hp_1, hp_2):\n",
    "    reference_split = clean_array(reference.split(\" \"))\n",
    "    \n",
    "    hp_1_split = clean_array(hp_1.split(\" \"))\n",
    "    hp_2_split = clean_array(hp_2.split(\" \"))\n",
    "    \n",
    "    BLEUscore_hp_1 = nltk.translate.bleu_score.sentence_bleu([reference_split], hp_1_split)\n",
    "    BLEUscore_hp_2 = nltk.translate.bleu_score.sentence_bleu([reference_split], hp_2_split)\n",
    "    print(\"Example \", index)\n",
    "    print(\"Fine-tuned score: \", BLEUscore_hp_1)\n",
    "    print(\"Base score: \", BLEUscore_hp_2)\n",
    "\n",
    "    print(\"******************\")\n",
    "\n",
    "    return BLEUscore_hp_1, BLEUscore_hp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('llama_32_1b_evaluation_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = []\n",
    "\n",
    "for el in evaluation_set:\n",
    "    BLEUscore_fine_tuned, BLEUscore_base = calculate_score(\n",
    "        el[\"index\"],\n",
    "        el[\"target_answer\"],\n",
    "        el[\"fine_tuned_answer\"],\n",
    "        el[\"base_answer\"])\n",
    "    \n",
    "    data.append([el[\"index\"], BLEUscore_fine_tuned, BLEUscore_base])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"index\", \"Fine-tuned score\", \"Base score\"])\n",
    "\n",
    "df[\"Fine-tuned score\"] = df[\"Fine-tuned score\"].astype(float)\n",
    "df[\"Base score\"] = df[\"Base score\"].astype(float)\n",
    "\n",
    "df.to_csv(\"./llama_32_1b_bleu_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"llama_32_1b_bleu_scores.csv\")\n",
    "\n",
    "data1 = df['Fine-tuned score']\n",
    "data2 = df['Base score']\n",
    "\n",
    "combined_data = pd.DataFrame({\n",
    "    'Fine-tuned model scores': data1,\n",
    "    'Base model scores': data2\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_data)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Distribution of Scores: Fine-tuned vs Base Model')\n",
    "\n",
    "plt.savefig('./images/llama_32_1b_bleu_scores.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1\n",
    "\n",
    "Training Arguments:\n",
    "* `epochs`: 1\n",
    "* `per_device_train_batch_size`: 2\n",
    "* `per_device_test_batch_size`: 2\n",
    "* `gradient_accumulation_steps`: 2\n",
    "* `gradient_checkpointing`: True\n",
    "\n",
    "These results are obtained by fine-tuning on 6000 rows in total, where 3000 rows of the dataset were duplicated for having both therminology on the `CONEQUENCE_DEFECT` and `CORRECTIVE_ACTION`.\n",
    "\n",
    "Total time for fine-tuning:\n",
    "* `ml.g5.12xlarge`: ~42 minutes on 4 GPUs\n",
    "\n",
    "Evaluation is performed on 10 rows extracted from the original dataset and not contained in the dataset used for the fine-tuning.\n",
    "\n",
    "BLEU score is performed with the fine-tuned model hosted on Amazon SageMaker, with an `ml.g5.4xlarge`, and the base model in Amazon Bedrock.\n",
    "\n",
    "Base model: `LLama-3.1 8B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_table.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that in most cases, the fine-tuned model seems to be outperforming the base model. The fine-tuned model appears to be more consistent in its performance.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n",
    "\n",
    "***\n",
    "\n",
    "Base model: `LLama-3 70B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_table_70.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_70.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that in most cases, the fine-tuned model seems to be outperforming the base model. The fine-tuned model appears to be more consistent in its performance.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    normalized_distance = 1 - (distance / max_len) if max_len > 0 else 1\n",
    "    return normalized_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('llama_32_1b_evaluation_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = [] \n",
    "\n",
    "for el in evaluation_set:\n",
    "    print(\"Example \", el[\"index\"])\n",
    "    score_fine_tuned = levenshtein_similarity(el[\"fine_tuned_answer\"], el[\"target_answer\"])\n",
    "    print(\"Fine-tune score: \", score_fine_tuned)\n",
    "    score_base = levenshtein_similarity(el[\"base_answer\"], el[\"target_answer\"])\n",
    "    print(\"Base score: \", score_base)\n",
    "    print(\"******************\")\n",
    "\n",
    "    data.append([el[\"index\"], score_fine_tuned, score_base])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"index\", \"Fine-tuned score\", \"Base score\"])\n",
    "\n",
    "df[\"Fine-tuned score\"] = df[\"Fine-tuned score\"].astype(float)\n",
    "df[\"Base score\"] = df[\"Base score\"].astype(float)\n",
    "\n",
    "df.to_csv(\"./llama_32_1b_levenshtein_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"llama_32_1b_levenshtein_scores.csv\")\n",
    "\n",
    "data1 = df['Fine-tuned score']\n",
    "data2 = df['Base score']\n",
    "\n",
    "combined_data = pd.DataFrame({\n",
    "    'Fine-tuned model scores': data1,\n",
    "    'Base model scores': data2\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_data)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Distribution of Scores: Fine-tuned vs Base Model')\n",
    "\n",
    "plt.savefig('./images/llama_32_1b_levenshtein_scores.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Levenshtein Score Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "\n",
    "Training Arguments:\n",
    "* `epochs`: 1\n",
    "* `per_device_train_batch_size`: 2\n",
    "* `per_device_test_batch_size`: 1\n",
    "* `gradient_accumulation_steps`: 2\n",
    "* `gradient_checkpointing`: True\n",
    "\n",
    "These results are obtained by fine-tuning on 6000 rows in total, where 3000 rows of the dataset were duplicated for having both therminology on the `CONEQUENCE_DEFECT` and `CORRECTIVE_ACTION`.\n",
    "\n",
    "Total time for fine-tuning:\n",
    "* `ml.g5.12xlarge`: ~39 minutes on 4 GPUs\n",
    "\n",
    "Evaluation is performed on 10 rows extracted from the original dataset and not contained in the dataset used for the fine-tuning.\n",
    "\n",
    "Levenshtein score is performed with the fine-tuned model hosted on Amazon SageMaker, with an `ml.g5.4xlarge`, and the base model in Amazon Bedrock.\n",
    "\n",
    "Base model: `LLama-3.1 8B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_table.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that the fine-tuned model is clearly outperforming the base model across almost all examples. This suggests that the fine-tuning process has been quite effective in improving the model's accuracy for this specific task.\n",
    "\n",
    "In the Normalized Levenshtein distance, the range is from 0 to 1, where closer to 0 means better performance. The fine-tuned model often achieves scores closer to 0, indicating higher accuracy.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n",
    "\n",
    "***\n",
    "\n",
    "Base model: `LLama-3 70B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_table_70.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_70.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that the fine-tuned model is clearly outperforming the base model across almost all examples. This suggests that the fine-tuning process has been quite effective in improving the model's accuracy for this specific task.\n",
    "\n",
    "In the Normalized Levenshtein distance, the range is from 0 to 1, where closer to 0 means better performance. The fine-tuned model often achieves scores closer to 0, indicating higher accuracy.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
