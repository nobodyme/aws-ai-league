{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment of Fine-Tuned Llama 3.1 8B in Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to deploy the fine-tuned model from the notebook [01_llama-3.1-8b-qlora-sft.ipynb](./01_llama-3.1-8b-qlora-sft.ipynb) in Amazon Bedrock by using the [Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html) functionality.\n",
    "\n",
    "---\n",
    "\n",
    "JupyterLab Instance Type: ml.t3.medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libriaries, including the Hugging Face libraries, and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q -U boto3\n",
    "%pip install -q -U botocore\n",
    "%pip install -q -U Levenshtein\n",
    "%pip install -q -U scikit-learn==1.5.1\n",
    "%pip install -q -U seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip generated model.tar.gz\n",
    "\n",
    "Amazon Bedrock Custom Model Import provides two different import options:\n",
    "1. From S3 bucket\n",
    "2. From an Amazon SageMaker Model\n",
    "\n",
    "In the following cells, we are going to upload in S3 the uncompressed model generated in the notebook [01_llama-3.1-8b-qlora-sft.ipynb](./01_llama-3.1-8b-qlora-sft.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T18:18:47.299354Z",
     "start_time": "2023-09-02T18:18:47.059439Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T18:18:47.629480Z",
     "start_time": "2023-09-02T18:18:47.609236Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_prefix = f\"train-{model_id.split('/')[-1].replace('.', '-')}-auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefix):\n",
    "    import boto3\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    search_response = sagemaker_client.search(\n",
    "        Resource='TrainingJob',\n",
    "        SearchExpression={\n",
    "            'Filters': [\n",
    "                {\n",
    "                    'Name': 'TrainingJobName',\n",
    "                    'Operator': 'Contains',\n",
    "                    'Value': job_name_prefix\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'TrainingJobStatus',\n",
    "                    'Operator': 'Equals',\n",
    "                    'Value': \"Completed\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1)\n",
    "    \n",
    "    return search_response['Results'][0]['TrainingJob']['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = get_last_job_name(job_prefix)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donwload fine-tuned Peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T18:18:58.270521Z",
     "start_time": "2023-09-02T18:18:48.067777Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.download_file(bucket_name, f\"{job_name}/{job_name}/output/model.tar.gz\", \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T18:18:59.409318Z",
     "start_time": "2023-09-02T18:18:58.272830Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ./model && mkdir -p ./model && tar -xf model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload uncompressed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('./model'):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        print(f'Sending {file_path} to S3...')\n",
    "        s3.Bucket(bucket_name).upload_file(\n",
    "            file_path,\n",
    "            f\"{job_name}/{job_name}/model/{file}\"\n",
    "        )\n",
    "        print(f'{file_path} sent successfully to S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf model.tar.gz ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Fine-Tuned model\n",
    "\n",
    "Now follow the steps from the link below to continue to import this model\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Import](./images/bedrock-llama31-import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Import Job](./images/bedrock-llama31-job-name.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model location](./images/bedrock-llama31-model-settings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bedrock role](./images/bedrock-llama31-role.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Fine-tuned model vs. Base model\n",
    "\n",
    "We are going to evaluate the fine-tuned model and the base model on two metrics:\n",
    "* BLEU Score\n",
    "* Accuracy score with Levenshtein distance\n",
    "\n",
    "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\n",
    "\n",
    "\n",
    "Normalized Levenshtein distance is an algorithm for evaluating accuracy degree of how close the calculated or measured values are to the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Amazon Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your imported model ARN\n",
    "fine_tuned_model_id = \"<IMPORTED_MODEL_ARN>\"\n",
    "bedrock_model_id = \"meta.llama3-70b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./sample_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, valid = train_test_split(train, test_size=10, random_state=42)\n",
    "\n",
    "print(\"Number of validation elements: \", len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "evaluation_set = []\n",
    "\n",
    "for index, row in valid.iterrows():\n",
    "    print(\"Example \", index)\n",
    "\n",
    "    ## Generate response with the fine-tuned model\n",
    "\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>These are the information related to the defect:\\nManufacturer: {row['MFGNAME']}\\nComponent: {row['COMPNAME']}\\nDescription of the defect: {row['DESC_DEFECT']}\\n\\n\\nWhat are the consequences of defect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=fine_tuned_model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Generated response with fine-tuned model: {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "    response_fine_tuned = json.loads(response[\"body\"].read())\n",
    "\n",
    "    response_fine_tuned = response_fine_tuned[\"generation\"]\n",
    "\n",
    "    print(response_fine_tuned)\n",
    "\n",
    "    ## Generate response with the base model\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        These are the information related to the defect:\n",
    "        Manufacturer: {row['MFGNAME']}\n",
    "        Component: {row['COMPNAME']}\n",
    "        Description of a defect:\n",
    "        {row['DESC_DEFECT']}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt},\n",
    "                        {\"text\": \"What are the consequences?\"}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=bedrock_model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.2,\n",
    "            \"topP\": 0.9,\n",
    "            \"maxTokens\": 200\n",
    "        }\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Generated response with base model: {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "    response_base = response['output']['message'][\"content\"][0][\"text\"]\n",
    "    print(response_base)\n",
    "\n",
    "    evaluation_set.append({\n",
    "        \"index\": index,\n",
    "        \"target_answer\": row[\"CONEQUENCE_DEFECT\"],\n",
    "        \"fine_tuned_answer\": response_fine_tuned,\n",
    "        \"base_answer\": response_base\n",
    "    })\n",
    "\n",
    "    print(\"******************\")\n",
    "\n",
    "with open(\"llama_32_1b_evaluation_dataset.json\", \"w\") as f:\n",
    "    json.dump(evaluation_set, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def tokenize(text):\n",
    "    # lowercase, keep letters/numbers/apostrophes\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
    "\n",
    "sm = SmoothingFunction().method3  # good sentence-level smoother\n",
    "\n",
    "with open('llama_32_1b_evaluation_dataset.json', 'r') as f:\n",
    "    evaluation_set = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for el in evaluation_set:\n",
    "    ref = tokenize(el[\"target_answer\"])\n",
    "    hyp_f = tokenize(el[\"fine_tuned_answer\"])\n",
    "    hyp_b = tokenize(el[\"base_answer\"])\n",
    "\n",
    "    bleu2_f = sentence_bleu([ref], hyp_f, weights=(0.5,0.5), smoothing_function=sm)\n",
    "    bleu2_b = sentence_bleu([ref], hyp_b, weights=(0.5,0.5), smoothing_function=sm)\n",
    "\n",
    "    bleu4_f = sentence_bleu([ref], hyp_f, smoothing_function=sm)  # default 4-gram weights\n",
    "    bleu4_b = sentence_bleu([ref], hyp_b, smoothing_function=sm)\n",
    "\n",
    "    rows.append([el[\"index\"], bleu2_f, bleu2_b, bleu4_f, bleu4_b])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"index\", \"Fine BLEU-2\", \"Base BLEU-2\", \"Fine BLEU-4\", \"Base BLEU-4\"])\n",
    "df.to_csv(\"./llama_32_1b_bleu_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"llama_32_1b_bleu_scores.csv\")\n",
    "\n",
    "data1 = df['Fine-tuned score']\n",
    "data2 = df['Base score']\n",
    "\n",
    "combined_data = pd.DataFrame({\n",
    "    'Fine-tuned model scores': data1,\n",
    "    'Base model scores': data2\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_data)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Distribution of Scores: Fine-tuned vs Base Model')\n",
    "\n",
    "plt.savefig('./images/llama_32_1b_bleu_scores.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1\n",
    "\n",
    "Training Arguments:\n",
    "* `epochs`: 1\n",
    "* `per_device_train_batch_size`: 2\n",
    "* `per_device_test_batch_size`: 2\n",
    "* `gradient_accumulation_steps`: 2\n",
    "* `gradient_checkpointing`: True\n",
    "\n",
    "These results are obtained by fine-tuning on 6000 rows in total, where 3000 rows of the dataset were duplicated for having both therminology on the `CONEQUENCE_DEFECT` and `CORRECTIVE_ACTION`.\n",
    "\n",
    "Total time for fine-tuning:\n",
    "* `ml.g5.12xlarge`: ~42 minutes on 4 GPUs\n",
    "\n",
    "Evaluation is performed on 10 rows extracted from the original dataset and not contained in the dataset used for the fine-tuning.\n",
    "\n",
    "BLEU score is performed with the fine-tuned model hosted on Amazon SageMaker, with an `ml.g5.8xlarge`, and the base model in Amazon Bedrock.\n",
    "\n",
    "Base model: `LLama-3.1 8B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_table.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that in most cases, the fine-tuned model seems to be outperforming the base model. The fine-tuned model appears to be more consistent in its performance.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n",
    "\n",
    "***\n",
    "\n",
    "Base model: `LLama-3 70B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_table_70.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_bleu_scores_70.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that in most cases, the fine-tuned model seems to be outperforming the base model. The fine-tuned model appears to be more consistent in its performance.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    normalized_distance = 1 - (distance / max_len) if max_len > 0 else 1\n",
    "    return normalized_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('llama_32_1b_evaluation_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = [] \n",
    "\n",
    "for el in evaluation_set:\n",
    "    print(\"Example \", el[\"index\"])\n",
    "    score_fine_tuned = levenshtein_similarity(el[\"fine_tuned_answer\"], el[\"target_answer\"])\n",
    "    print(\"Fine-tune score: \", score)\n",
    "    score_base = levenshtein_similarity(el[\"base_answer\"], el[\"target_answer\"])\n",
    "    print(\"Base score: \", score)\n",
    "    print(\"******************\")\n",
    "\n",
    "    data.append([el[\"index\"], score_fine_tuned, score_base])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"index\", \"Fine-tuned score\", \"Base score\"])\n",
    "\n",
    "df[\"Fine-tuned score\"] = df[\"Fine-tuned score\"].astype(float)\n",
    "df[\"Base score\"] = df[\"Base score\"].astype(float)\n",
    "\n",
    "df.to_csv(\"./llama_32_1b_levenshtein_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"llama_32_1b_levenshtein_scores.csv\")\n",
    "\n",
    "data1 = df['Fine-tuned score']\n",
    "data2 = df['Base score']\n",
    "\n",
    "combined_data = pd.DataFrame({\n",
    "    'Fine-tuned model scores': data1,\n",
    "    'Base model scores': data2\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_data)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Distribution of Scores: Fine-tuned vs Base Model')\n",
    "\n",
    "plt.savefig('./images/llama_32_1b_levenshtein_scores.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Levenshtein Score Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "\n",
    "Training Arguments:\n",
    "* `epochs`: 1\n",
    "* `per_device_train_batch_size`: 2\n",
    "* `per_device_test_batch_size`: 1\n",
    "* `gradient_accumulation_steps`: 2\n",
    "* `gradient_checkpointing`: True\n",
    "\n",
    "These results are obtained by fine-tuning on 6000 rows in total, where 3000 rows of the dataset were duplicated for having both therminology on the `CONEQUENCE_DEFECT` and `CORRECTIVE_ACTION`.\n",
    "\n",
    "Total time for fine-tuning:\n",
    "* `ml.g5.12xlarge`: ~39 minutes on 4 GPUs\n",
    "\n",
    "Evaluation is performed on 10 rows extracted from the original dataset and not contained in the dataset used for the fine-tuning.\n",
    "\n",
    "Levenshtein score is performed with the fine-tuned model hosted on Amazon SageMaker, with an `ml.g5.8xlarge`, and the base model in Amazon Bedrock.\n",
    "\n",
    "Base model: `LLama-3.1 8B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_table.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that the fine-tuned model is clearly outperforming the base model across almost all examples. This suggests that the fine-tuning process has been quite effective in improving the model's accuracy for this specific task.\n",
    "\n",
    "In the Normalized Levenshtein distance, the range is from 0 to 1, where closer to 0 means better performance. The fine-tuned model often achieves scores closer to 0, indicating higher accuracy.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n",
    "\n",
    "***\n",
    "\n",
    "Base model: `LLama-3 70B Instruct`\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_table_70.png)\n",
    "\n",
    "##### BLEU Scores graph\n",
    "\n",
    "![BLEU Scores Table](./images/llama_32_1b_levenshtein_scores_70.png)\n",
    "\n",
    "By comparing the scores in the \"Fine-tuned Score\" and \"Base Score\" columns, we can assess the performance improvement (or degradation) achieved by fine-tuning the model on the specific task or domain.\n",
    "\n",
    "The analysis suggest that the fine-tuned model is clearly outperforming the base model across almost all examples. This suggests that the fine-tuning process has been quite effective in improving the model's accuracy for this specific task.\n",
    "\n",
    "In the Normalized Levenshtein distance, the range is from 0 to 1, where closer to 0 means better performance. The fine-tuned model often achieves scores closer to 0, indicating higher accuracy.\n",
    "\n",
    "Possible improvements:\n",
    "* Examples repetition: Provide similar examples for improving further improving the vocabulary of the fine-tuned model\n",
    "* Increse the number of epochs\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
